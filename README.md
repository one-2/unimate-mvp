# UniMate Backend (MVP)

UniMate was a real-world MVP built by a second-year computer science student at UNSW. It aggregated and processed event data from student groups across the internet, using distributed proxies for scraping and GPT-based classification for semantic enrichment. The system ran live for over 1,500 users and was designed to validate product-market fit with minimal engineering overhead.

ğŸ§Š This repo represents a cleaned and frozen version of the final push of the UniMate MVP. It is not under active development.

ğŸ“ [Read the retrospective](https://stelliott.online/2025/03/31/unimate-retrospective-of-a-first-time-engineering-founder/) for the full breakdown of the design process, tradeoffs, and lessons learned.

---

## ğŸš€ What It Does

- Scrapes club event data using Infatica proxies (residential/mobile IPs).
- Parses inconsistent HTML structures with custom logic and resilient regex-based strategies.
- Enriches raw event data using GPT-3.5 for multi-label classification.
- Outputs structured CSVs and binary dumps.
- Supports manual overrides, partial restarts, and cached recovery.
- Uses a no-code frontend for minimal deployment complexity.

---

## ğŸ§± Architecture

scraper/ \
â”œâ”€â”€ Scraper.py â†’ Entry point for scraping club and event pages \
â”œâ”€â”€ Scripts.py â†’ Modular extractors for embedded script data \
â”œâ”€â”€ InfaticaRequests.py â†’ Proxy-based HTTP layer 

processor/ \
â”œâ”€â”€ Tagger.py â†’ GPT-based multi-label classifier 

utils/ \
â”œâ”€â”€ Parser.py â†’ HTML substring and regex extraction \
â”œâ”€â”€ Utils.py â†’ Date handling, sleep logic, URL checks 

io/ \
â”œâ”€â”€ Filesystem.py â†’ Handles all folder and file structure \
â”œâ”€â”€ Input.py â†’ Reads from saved files and dumps \
â”œâ”€â”€ Output.py â†’ Writes data to disk (CSV, binary) 

config/ \
â”œâ”€â”€ Config.py â†’ Prompts and constants \
â”œâ”€â”€ Env.py â†’ Shared imports 

core/ \
â”œâ”€â”€ Interface.py â†’ CLI interaction and control logic \
â”œâ”€â”€ Debug.py â†’ Test utilities 


---

## âš™ï¸ Setup

> **Warning:** This repo contains *legacy MVP code*. It was designed for fast iteration, not safety or polish.

1. **Clone the repo**
2. Set your `infatica_api_key` in `Config.py`  
   *(you should remove or mock this if publishing your version)*  
3. Run `Interface.py` to begin CLI-driven scraping and tagging.

Youâ€™ll need:
- Python 3.9+
- OpenAI API key (for `Tagger.py`)
- A valid Infatica account if testing live scraping

---

## ğŸ§  Design Patterns Used

- **Pipeline Pattern** â€“ Cleanly separated data stages: scraping â†’ processing â†’ output
- **Strategy Pattern** â€“ GPT taggers encapsulated as binary classifiers
- **Factory Pattern (lightweight)** â€“ Filesystem handles path generation
- **CLI Controller** â€“ User input drives pipeline selection
- **Observer-like Logging** â€“ Post-mortem debugging with HTML dumps

---

## ğŸ§ª Known Limitations

- No formal test suite. It's all manual, via CLI and prints
- API keys were originally stored in plain text
- Minimal error handling and no logging framework
- Single-threaded, non-concurrent scraping
- Not intended for production deployment

---

## ğŸ“š Related Reading

For context, lessons learned, and a full breakdown of tradeoffs:  
ğŸ‘‰ [UniMate: Retrospective of a First-Time Engineering Founder](https://stelliott.online/2025/03/31/unimate-retrospective-of-a-first-time-engineering-founder/)

---

## ğŸ§‘â€ğŸ’» Author

Stephen Elliott  
[Sydney, 2023]
